# Summarizing podcast episodes using topic modeling

In the recent years podcasts have become an exceedingly popular medium for content creation and dissemination. Not only are there more listeners but also more content creators. Due to the availability of a huge variety of podcasts, it is evident that a more nuanced/sophisticated framework is needed to categorize the podcast episodes than a simple genre-based classification scheme. To that end, topic modelling is a viable candidate methodology for finding the key topics that describe a collection of podcast episodes (equivalent to a corpus of documents). 

The following streamlit application is a platform where a user (any podcast enthusiast) can choose:
1) a genre from the given collection, 
2) average duration of the episode, and 
3) a set of keywords that they're looking for as describing an episode (words seperated by commas). 

Based on this information, the models suggest a maximum of three episodes. The output consists of the podcast name, episode name, episode duration, description of the episodes and the url of the podcast episodes (if available).

https://podcast-episodes-deepdive-app.herokuapp.com


The steps and tools used to develop the app are described below.

## Step 1: Data
- The data is sourced from the transcript files generated by machine translation of the audio files corresponding to podcast episodes. This data was released by Spotify under the folllowing name: "Spotify Podcasts 2020 dataset". Access can be obtained by following the instructions provided at: https://podcastsdataset.byspotify.com/

## Step 2: Model development & visualization
- The LDA model (Latent Dirichlet Allocation) is used for topic modelling, and the word2vec model is used to extract most similar topic to the keywords entered by the user. We use gensim package for model development. Here's the link to the github repository containing scripts, models and visualization html files: https://github.com/sgmark3/Topic-modeling-of-podcast-episodes 
- We use pyLDAvis to deploy an interative visualisation tool deployed as an html page. 
- The visualization shows: 1) the distribution of terms across the transcripts corresponding to different topics, and the distribution of the same set of terms across the entire corpus, and 2) a 2D embedding of the corpus (collection of transcripts in our case) in the space of two principle components. These distributions can be varied by choosing a value of the relevancy parameter through an interactive slider tool.
- Here's a reference for further exploration: https://pyldavis.readthedocs.io/en/latest/readme.html 

## Step 3: Web application
- We use streamlit to create a web-application, and we deploy it on heroku.
- As mentioned above, the web application asks the user to choose: 1) a genre from the given collection; 2) average duration of the episode; 3) a set of keywords that they're looking for as describing an episode (words seperated by commas). 
- Based on this information, the models suggest a maximum of three episodes. The output consists of the podcast name, episode name, episode duration, description of the episodes and the url of the podcast episodes (if available).
